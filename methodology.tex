\section{Methodology}
In our evaluation, we will be answering the following questions:

\begin{itemize}
\item How does Dynamic Heap Resizing improve performance compared to static DRAM partitioning?
\item What is the impact of dynamic resizing on garbage collection overhead and application execution time?
\item How robust is the Dynamic Heap Resizer under different lucene workloads?
\end{itemize}

All experiments were conducted on sith1 a dedicated server equipped with two Intel(R) Xeon(R) CPU E5-2630 v3 processors with 16 physical cores with hyperthreading enabled (32 hardware threads in total). The machine has 256 GB of DDR4 DRAM and a 1.5 TB Samsung NVMe SSD (SAMSUNG MZPLJ1T6HBJR-00007) mounted at /mnt/spark for the H2 file, which we used for storing datasets and experimental results. The system runs Ubuntu 20.04.6 LTS with Linux kernel version 6.6.94. For each experiment the DRAM capacity has been limited by using cgroups.
We evaluate our system using a 300 GB real-world Lucene dataset containing 52.6 billion words. We preprocess this dataset by performing a complete word count and filtering out stop words, single-character words, and non-English words. Based on frequency distributions, words are categorized into three groups: high-frequency (top 1\% of words, accounting for ~80\% of total occurrences), medium-frequency (mid-range words), and low-frequency (91\% of words, contributing only ~1\% of occurrences). This classification models different levels of I/O load, as high-frequency words lead to significantly higher document access rates compared to low-frequency words.

We construct queries of two sizes: small queries retrieving the top 50 results and large queries retrieving up to 500,000 results. Large queries place higher pressure on the managed heap, while frequency classes primarily influence I/O cost. Our evaluation includes three workloads:
M1, high-frequency small queries (HS);
M2, high-frequency large queries (HL); and
M3, a mixed workload consisting of medium-frequency small (MS) and medium-frequency large (ML) queries, reflecting realistic deployments where both I/O load and heap pressure vary dynamically.

For baseline comparison, we evaluate three configurations. First, we run Lucene using TeraHeap with a static DRAM partition, allocating 50\% of the 40 GB DRAM budget to H1 (managed heap) and 50\% to H2 (page cache backed by NVMe storage), representing a typical balanced setup when no detailed tuning is performed. Second, we run Lucene with hand-tuned ideal configurations, where the DRAM allocation between H1 and H2 is manually optimized to minimize execution time and garbage collection overheads for each benchmark. Finally, we evaluate our Dynamic Heap Resizer, which automatically adjusts the size of H1 at runtime. This experimental setup enables us to compare the effectiveness of dynamic DRAM partitioning against fixed static configurations and manually optimized ideal baselines.
