\section{Design}

% eliminate bullets, make as paragraph

\subsection{Overview}

The goal of FlexHeap is to dynamically split DRAM between the primary heap (H1)
and the page cache used by TeraHeap’s secondary heap (H2), to reduce garbage
collection (GC) and I/O overheads. It monitors runtime behavior in short
intervals and uses a finite state machine (FSM) to decide when to grow or
shrink the main heap. GC and I/O costs are translated into lost CPU time and
then compared to determine the optimal resizing action. Expansion decisions are
scaled based on CPU usage and are bound between limits to prevent overly
aggressive or conservative heap growth. Shrink operations are more aggressive,
aiming to release memory back to the page cache by reclaiming up to 50\% of the
currently free heap regions. This design allows FlexHeap to respond to workload
changes and improve system performance dynamically.

% \subsection{Interval-Based Measurement Model}
\subsection{Estimating the impact of GC cost and I/O stalls}

All resizing decisions are made at the end of an interval. An interval is
defined as the time between two consecutive stop-the-world garbage collections
Young GCs, Mixed GCs or Full GCs). These intervals are recurring measurement windows in which both the
application threads (mutator) and the garbage collector (GC) can perform
working tasks. Within each interval, we estimate the cost of garbage collection and I/O activity,
to provide sufficient insight into system behavior to guide our dynamic resizing policy.
% \note{jk: our measurements are not accurate but they are an
% 	estimation of the costs. so you need to defind that we estimate the GC cost and
% 	I/O cost.}

During an interval, we track runtime metrics including:
\begin{itemize}
	\item The total CPU time lost due to GC, including stop-the-world pauses, concurrent GC threads, and refinement threads.
	\item The I/O overhead introduced by page cache while perfoming accesses to TeraHeap's secondary heap (H2).
\end{itemize}
	
\paragraph{Measuring STW pauses:} In these events
all application threads (mutators) are paused so that the JVM can exclusively perform
GC-related work. In order to calculate the time overheads produced by G1’s stop-the-world (STW) phases,
we measure the time spent inside key G1 safepoint functions.
Specifically we place timers around safepoint phases of the G1 garbage collector.
These include the stop-the-world pause for young and mixed collections, the concurrent
marking phase where the reachability of live objects is finalized and the full GC path
that reclaims the entire heap when necessary.

\paragraph{Measuring Concurrent GC threads:} These threads operate in the background,
concurrently with application execution.
Hence, in order to account for compute cycles lost to concurrent GC threads,
we must determine whether GC activity actually interferes with
application (mutator) execution. This interference only occurs when the combined number of
mutator threads and concurrent GC threads exceeds the number of available
CPU cores. In such cases, concurrent GC threads compete with the application
for CPU time, causing actual slowdown.

If the total mutator and GC threads is less or equal than the number of cores, the concurrent
GC threads are able to run on otherwise idle CPUs. In this case, we assume that
CPUs remaining idle do not cause performance loss.

Even under conditions where concurrent GC threads and mutator threads compete for CPU resources we avoid attributing all GC time as overhead.
We first subtract the amount of time GC threads could have used on idle cores estimated
as the number of free cores \texttt{\#cores} $-$ \texttt{\#mutators} $x$ \texttt{interval\_duration} .
The remaining time is treated as actual interference with the
application and is counted as GC overhead.

\paragraph{Measuring Refinement threads:} Refinement threads are a part of concurrent collection,
so the full runtime of refinement threads is interpreted as GC overhead. This is because refinement thread
activity directly supports concurrent GC phases and typically runs in the background,
contributing 100\% of their execution time to GC-related compute loss.


% \begin{itemize}
% 	\item \textbf{Measuring Stop-the-world (STW) pauses:} In these events
% 	      all application threads (mutators) are paused so that the JVM can exclusively perform
% 	      GC-related work. In order to calculate the time overheads produced by G1’s stop-the-world (STW) phases,
% 	      we measure the time spent inside key G1 safepoint functions.
% 	      Specifically we place timers around safepoint phases of the G1 garbage collector.
% 	      These include the stop-the-world pause for young and mixed collections, the concurrent
% 	      marking phase where the reachability of live objects is finalized and the full GC path
% 	      that reclaims the entire heap when necessary.
%
% 	      After obtaining the time spent in the safepoint functions, we translate it into CPU overhead, by using the following formula:
% 	      \[
% 		      \text{cpu\_time\_spent} += \text{gc\_pause\_time} \times \min(\text{mutators}, \text{cores})
% 	      \]
% 	      This formula estimates CPU cycles lost due to GC pauses by considering both the duration of the pauses and the level of available parallelism.
% 	      Although mutator threads are inactive during STW events, they represent potential application work that is delayed.
% 	      By only counting the threads that could actually run at the same time, we get a more accurate and realistic estimate of how much the application was slowed down.
%
% 	\item \textbf{Measuring Concurrent GC threads:} These threads operate in the background,
% 	      concurrently with application execution.
% 	      Hence, in order to account for compute cycles lost to concurrent GC threads,
% 	      we must determine whether GC activity actually interferes with
% 	      application (mutator) execution. This interference only occurs when the combined number of
% 	      mutator threads and concurrent GC threads exceeds the number of available
% 	      CPU cores. In such cases, concurrent GC threads compete with the application
% 	      for CPU time, causing actual slowdown.
%
% 	      If the total mutator and GC threads is less or equal than the number of cores, the concurrent
% 	      GC threads are able to run on otherwise idle CPUs. In this case, we assume that
% 	      CPUs remaining idle do not cause performance loss.
%
% 	      Even under conditions where concurrent GC threads and mutator threads compete for CPU resources we avoid attributing all GC time as overhead.
% 	      We first subtract the amount of time GC threads could have used on idle cores estimated
% 	      as the number of free cores \texttt{\#cores} $-$ \texttt{\#mutators} $x$ \texttt{interval\_duration} .
% 	      The remaining time is treated as actual interference with the
% 	      application and is counted as GC overhead.
%
% 	\item \textbf{Measuring Refinement threads:} Refinement threads are a part of concurrent collection,
% 	      so the full runtime of refinement threads is interpreted as GC overhead. This is because refinement thread
% 	      activity directly supports concurrent GC phases and typically runs in the background,
% 	      contributing 100\% of their execution time to GC-related compute loss.
%
% \end{itemize}

To measure garbage collection overhead, our system tracks the cumulative CPU time spent in each of these three components.
This data is collected in each interval and used by FlexHeap to determine whether GC-related costs are increasing or decreasing.

\subsubsection{I/O Monitoring via eBPF Library}  % make sub sub section

To accurately capture the I/O overhead produced by page‑cache accesses and
object transfers in the secondary heap (H2), we used an existing TeraHeap
eBPF‑based monitoring library. This library runs in kernel space and tracks
events such as page faults and read‑and‑write operations of mapped regions.
Whenever an object accessed in H2 triggers a page‑cache miss or eviction, the
library reports the data movement, which we convert into CPU‑equivalent lost
cycles. By integrating this eBPF library, I/O stalls can be translated into
equivalent CPU time lost, enabling a direct comparison with GC overheads during
each interval.

\subsection{Heap resizing decision making} % rename to somethging like decision making, or dram division...

Upon completion of an interval, FlexHeap collects the GC and I/O lost compute cycles and passes them through
a \textit{finite state machine (FSM)}. The FSM serves as a control mechanism
that reacts to both GC and I/O overhead percent changes and decides whether to adjust the DRAM allocation
between the heap (H1) and the page-cache. The FSM consists of 3 states:

Firstly, the \texttt{wait\_after\_grow} state, which represents a phase that occurs after
the system has increased the size of the heap (H1). Its purpose is to
to monitor whether the previous growth decision effectively
reduced overall overheads without destabilizing the system.

Each time the state is evaluated, FlexHeap retrieves runtime data from the G1 collector,
such as the current heap capacity, used and unused regions in bytes and the concurrent marking
start threshold (IHOP), to determine whether the heap is approaching
a GC initiation and whether there is available space for further expansions.

The decision logic follows several key checks:
\begin{itemize}
	\item If the total GC and I/O overhead has increased compared to the previous interval, the system
	      compares the relative growth rates of GC and I/O time. If GC overhead increased more than I/O,
	      FlexHeap interprets this as insufficient heap space and issues a \texttt{GROW\_HEAP} action,
	      provided the heap has not reached its maximum capacity.
	\item Conversely, if the I/O overhead increased more than the GC and the heap utilization is below 95\% of
	      total capacity, the system transitions to the \texttt{wait\_after\_shrink} state and issues a
	      \texttt{SHRINK\_HEAP} action, shrinking the heap and transferring DRAM to the page cache via cgroups.
	\item If none of these conditions are met, the system goes to a \texttt{no\_action\_state}.
\end{itemize}

Also, the systems uses IHOP as a safety condition. If the heap usage falls
below the IHOP limit and the current capacity is under the maximum allowed
size, the FSM remains in the grow state waiting to see if the system will
stabilize, without issuing any further grow actions.

The second state is the \texttt{wait\_after\_shrink}. Its purpose is to observe
how the system behaves after a heap shrinkage wihich consequently frees DRAM
back to the page cache. During this state, the FSM compares the garbage
collection (GC) with the I/O to determine whether the shrink operation had
reduced I/O pressure or, if further resizing actions are required.

The core logic of this state is as follows:
\begin{itemize}

	\item
	      If GC overhead have increased more than I/O, it indicates that shrinking the heap too aggressively,
	      led to increased GC activity. In this case, the system transitions to the \texttt{wait\_after\_grow} state
	      and performs a \texttt{GROW\_HEAP} action to give memory to the heap.

	\item If the I/O overhead increased the system stays in the \texttt{wait\_after\_shrink} state and issues further shrinks.
	      If the heap usage is still high (above 90\%), the FSM may also decide to grow again to relieve GC pressure.
\end{itemize}

Additionally, the state monitors total resident memory (rss) and page cache
usage. If the sum of resident memory and page cache usage falls below 80\% of
the configured DRAM limit, the FSM interprets is as (\texttt{IOSLACK}) and
issues another \texttt{SHRINK\_HEAP} action to allocate more memory to the page
cache. This allows to reclaim unused DRAM to relieve I/O when the system is
under light memory pressure.

If none of these conditions are triggered, the FSM
transitions to \texttt{no\_action}.

At last the FSM supports the \texttt{no\_action} state, which is the default state.
When the system enters this state, no DRAM resizing actions were performed.

At each interval, FlexHeap evaluates whether the total GC and I/O time has
changed. If the change is within a 5\% threshold, it is interpreted
as noise or a stable condition and the system remains in the \texttt{no\_action} state.

If the combined overhead has increased notably and the system is in this state, the FSM analyzes which overhead (GC or I/O) is
contributing more to the increase:
\begin{itemize}
	\item If the GC overhead has increased more than I/O, this suggests that H1 may need a grow action.
	      In that case, FlexHeap transitions to the \texttt{wait\_after\_grow} state and issues a \texttt{GROW\_HEAP} action,
	      unless the heap has already reached its maximum capacity.

	\item If the I/O overhead has increased more, FlexHeap checks if the heap usage is above 90\% of
	      its capacity. If not, the system transitions to \texttt{wait\_after\_shrink} and issues a \texttt{SHRINK\_HEAP}
	      action to return DRAM to the page cache.
\end{itemize}

If the total overhead has decreased compared to the previous interval, no resizing action is needed. In this case,
the system remains in the \texttt{no\_action} state.

\subsection{Heap Resizing Step}

When an expansion is required, the system calculates, by using the vanilla-G1's
resizing step, how much uncommitted memory is available. This is the memory
that has been reserved by the JVM but not yet used. A scaling factor is
computed based on the change in CPU overhead compared to the previous
measurement interval. This factor determines the aggressiveness of the heap
expansion and is bounded to prevent overly conservative or excessively
aggressive resizing. The final resize amount is then clamped within threshold
values to ensure it stays within bounds, large enough to make a adjustments,
but not to exhaust the remaining memory and create GC pressure.

In contrast, shrink decisions are more aggressive. Since returning DRAM to the
operating system's page cache can significantly reduce I/O delays for TeraHeap
workloads, shrink operations prioritize releasing unused memory. The amount of
memory to shrink is derived from the number of free heap regions—memory that
has already been committed but is not actively in use. A fixed scaling factor
(50\%), is then applied to determine how much of the free memory can be
reclaimed, ensuring that the main heap retains sufficient headroom for sudden
bursts of memory allocation without triggering excessive garbage collection.

