\section{Design}

\subsection{Interval-Based Measurement Model}

All resizing decisions are made at the end of an interval. An interval is defined as the time between 
two consecutive young garbage collections (Young GCs) in the G1 collector. These intervals are recurring 
measurement windows in which both the application threads (mutator) and the garbage collector (GC) can perform working tasks.

During an interval, we track runtime metrics including:
\begin{itemize}
  \item The total CPU time lost due to GC, including stop-the-world pauses, concurrent GC threads, and refinement threads.
  \item The I/O overhead introduced by page cache while perfoming accesses to TeraHeap's secondary heap (H2).
\end{itemize}

Due to intervals being short, they are ideal for constantly checking GC and I/O overheads 
without introducing significant delay or noise. The cumulative behavior across intervals is
then interpreted by a finite state machine (FSM), which issues grow or shrink 
actions based on the recorded runtime metrics.

\subsection{Overview}

% The goal of the design is to dynamically divisio the DRAM between the 
% primary heap (H1) the page-cache for the H2, based on the application's runtime behavior. 
% This is achieved through a feedback-driven mechanism that monitors performance and makes resizing decisions.
%
% At the end of each interval, the system invokes the \texttt{dram\_repartition()} function, which
% compares the total compute time lost to garbage collection (GC) and I/O operations to the previous interval.
% Based on the comparison the FSM issues resizing actions in order to reduce the overheads in future intervals.

% The \texttt{dram\_repartition()} function compares these two overheads to determine which source of 
% pressure is dominant. If GC overhead is higher, more DRAM is allocated to the on-heap region (H1). 
% If I/O pressure dominates, DRAM is released from H1 to allow more space for page cache and reduce evictions. 
% To prevent erratic or overly aggressive resizing, decisions are moderated by a finite state machine (FSM) that
% encodes cooldown periods and threshold conditions. This ensures that resizing actions are taken smoothly and 
% only when justified by sustained performance changes.
%
% In summary, our system continuously monitors runtime behavior, compares GC and I/O overheads at the end of
% each interval, and uses an FSM-guided policy to repartition DRAM between H1 and H2 in a responsive yet 
% controlled manner.
%


% In order for FlexHeap to effectively resize 
% the heap and give the appropriate amount of DRAM to the page 
% cache, it must accurately detect when performance bottlenecks occur
% from garbage collection (GC) or I/O. 
% In this section, we discuss FlexHeap's FSM for resizing actions,
% describe the different types of GC overhead scenarios and how they 
% are measured and explain how FlexHeap operates on sampling intervals to collect GC and I/O metrics.
%
%
\subsection{Calculation of GC overheads}
In G1, garbage collection overhead comes from three primary sources:

\begin{itemize}
  \item \textbf{Measuring Stop-the-world (STW) pauses:} In these events
  all application threads (mutators) are paused so that the JVM can exclusively perform 
  GC-related work. In order to calculate the time overheads produced by G1’s stop-the-world (STW) phases,
  we measure the time spent inside key G1 safepoint functions. 
  Specifically we place timers around safepoint phases of the G1 garbage collector. 
  These include the stop-the-world pause for young and mixed collections, the concurrent 
  marking phase where the reachability of live objects is finalized and the full GC path
  that reclaims the entire heap when necessary. 

  After obtaining the time spent in the safepoint functions, we translate it into CPU overhead, by using the following formula:
  \[
  \text{cpu\_time\_spent} += \text{gc\_pause\_time} \times \min(\text{mutators}, \text{cores})
  \]
  This formula estimates CPU cycles lost due to GC pauses by considering both the duration of the pauses and the level of available parallelism. 
  Although mutator threads are inactive during STW events, they represent potential application work that is delayed. 
  By only counting the threads that could actually run at the same time, we get a more accurate and realistic estimate of how much the application was slowed down. 

  \item \textbf{Measuring Concurrent GC threads:} These threads operate in the background,
  concurrently with application execution. 
  Hence, in order to account for compute cycles lost to concurrent GC threads, 
  we must determine whether GC activity actually interferes with 
  application (mutator) execution. This interference occurs only when 
  the system is oversubscribed. This occurs, when the combined number of 
  mutator threads and concurrent GC threads exceeds the number of available 
  CPU cores, due to context switching. In such cases, concurrent GC threads compete with the application 
  for CPU time, causing actual slowdown.

  If the system is not oversubscribed meaning that the number of mutator and GC
  threads together is less or equal than the number of cores, then the concurrent 
  GC threads are able to run on otherwise idle CPUs. In this case, we assume that
  CPUs remaining idle do not cause performance loss.

  Even in oversubscribed conditions, we avoid attributing all GC time as overhead.
  We first subtract the amount of time GC threads could have used on idle cores estimated 
  as the number of free cores \texttt{\#cores} $-$ \texttt{\#mutators} $x$ \texttt{interval\_duration} .
  The remaining time is treated as actual interference with the
  application and is counted as GC overhead.

  \item \textbf{Measuring Refinement threads:} Refinement threads are a part of concurrent collection, 
  so the full runtime of refinement threads is interpreted as GC overhead. This is because refinement thread 
  activity directly supports concurrent GC phases and typically runs in the background,
  contributing 100\% of their execution time to GC-related compute loss.

\end{itemize}

To measure garbage collection overhead, our system tracks the cumulative CPU tims spent in each of these three components. 
This data is collected in each interval and used by FlexHeap to determine whether GC-related costs are increasing or decreasing.

\subsection{I/O Monitoring via eBPF Library}

To accurately capture the I/O overhead produced by page‑cache accesses 
and object transfers in the secondary heap (H2), we used an existing 
eBPF‑based monitoring library. This library runs in kernel space and tracks events such as page faults and read‑and‑write
operations of mapped regions. Whenever an object accessed in H2 triggers a page‑cache miss or eviction, the library reports the 
data movement, which we convert into CPU‑equivalent lost cycles. 
By integrating this eBPF library, I/O stalls can be translated into equivalent 
CPU time lost, enabling a direct comparison with GC overheads during each interval.



\subsection{Heap resizing Finate State Machine}

After the end of an interval, FlexHeap collects the GC and I/O lost compute cycles and passes them through 
a \textit{finite state machine (FSM)}. The FSM serves as a control mechanism 
that reacts to both GC and I/O overhead percent changes and decides whether to adjust the DRAM allocation 
between the heap (H1) and the page-cache. The FSM consists of 3 states:

Firstly, the \texttt{wait\_after\_grow} state, which represents a phase that occurs after 
the system has increased the size of the heap (H1). Its purpose is to 
to monitor whether the previous growth decision effectively 
reduced overall overheads without destabilizing the system. 

Each time the state is evaluated, FlexHeap retrieves runtime data from the G1 collector, 
such as the current heap capacity, used and unused regions in bytes and the concurrent marking 
start threshold (IHOP), to determine whether the heap is approaching 
a GC initiation and whether there is available space for further expansions.

The decision logic follows several key checks:
\begin{itemize}
  \item If the total overhead has increased compared to the previous interval, the system
    compares the relative growth rates of GC and I/O time. If GC overhead increased more than I/O, 
    FlexHeap interprets this as insufficient heap space and issues a \texttt{GROW\_HEAP} action, 
    provided the heap has not reached its maximum capacity. 
  \item Conversely, if the I/O overhead increased more than the GC and the heap utilization is below 95\% of
    total capacity, the system transitions to the \texttt{wait\_after\_shrink} state and issues a 
    \texttt{SHRINK\_HEAP} action, shrinking the heap and transferring DRAM to the page cache via cgroups.
  \item If none of these conditions are met, the system goes to a \texttt{no\_action\_state}.
\end{itemize}

Also, the systems uses IHOP as a safety condition. If the heap usage falls below the 
IHOP limit and the current capacity is under the maximum allowed size, the FSM remains in the 
grow state waiting to see if the system will stabilize, without issuing any further grow actions.

The second state is the \texttt{wait\_after\_shrink}. Its purpose is to 
observe how the system behaves after a heap shrinkage wihich consequently frees DRAM back to the page cache. During this
state, the FSM compares the garbage collection (GC) with the I/O to determine whether
the shrink operation had reduced I/O pressure or, if further resizing actions are required.

The core logic of this state is as follows:
\begin{itemize}
  
  \item 
  If GC overhead have increased more than I/O, it indicates that shrinking the heap too aggressively,
  led to increased GC activity. In this case, the system transitions to the \texttt{wait\_after\_grow} state
  and performs a \texttt{GROW\_HEAP} action to give memory to the heap.
  
  \item If the I/O overhead increased the system stays in the \texttt{wait\_after\_shrink} state and issues further shrinks.
  If the heap usage is still high (above 90\%), the FSM may also decide to grow again to relieve GC pressure.
\end{itemize}

Additionally, the state monitors total resident memory (rss) and page cache usage.
If the sum of resident memory and page cache usage falls below 80\% 
of the configured DRAM limit, the FSM interprets is as (\texttt{IOSLACK}) and 
issues another \texttt{SHRINK\_HEAP} action to allocate more memory to the page cache. This allows 
to reclaim unused DRAM to relieve I/O when the system is under light memory pressure.

If none of these conditions are triggered, the FSM 
transitions to \texttt{no\_action}.

At last the FSM supports the \texttt{no\_action} state, which is the default state.
When the system enters this state, no DRAM resizing actions were performed.

At each interval, FlexHeap evaluates whether the total GC and I/O time (\texttt{gc\_time\_ms + io\_time\_ms}) has
changed. If the change is within a 5\% threshold, it is interpreted 
as noise or a stable condition and the system remains in the \texttt{no\_action} state.

If the combined overhead has increased notably and the system is in this state, the FSM analyzes which overhead (GC or I/O) is 
contributing more to the increase:
\begin{itemize}
  \item If the GC overhead has increased more than I/O, this suggests that H1 may need a grow action. 
  In that case, FlexHeap transitions to the \texttt{wait\_after\_grow} state and issues a \texttt{GROW\_HEAP} action, 
  unless the heap has already reached its maximum capacity.

  \item If the I/O overhead has increased more, FlexHeap checks if the heap usage is above 90\% of 
  its capacity. If not, the system transitions to \texttt{wait\_after\_shrink} and issues a \texttt{SHRINK\_HEAP} 
  action to return DRAM to the page cache.
\end{itemize}

If the total overhead has decreased compared to the previous interval, no resizing action is needed. In this case, 
the system remains in the \texttt{no\_action} state.
 
\subsection{Heap Resizing Step}

When a grow action is issued, FlexHeap invokes the \texttt{young\_collection\_expand\_amount()} function, 
which is a vanilla G1 resizing function.

The method begins by calculating the total amount of uncommitted memory which is the difference
between the reserved heap size (\texttt{max\_capacity()}) and the currently committed size 
(\texttt{capacity()}). This value represents the available space for heap expansion.

A scale factor is then computed using a delta cpu usage between the previous intervals and is bounded between two thresholds: \texttt{1.3} 
(minimum growth) and \texttt{4.2} (maximum growth). These growth values represent a x-axis value to a sigmoid
function which returns a scaled number based on how aggressive the heap will be expanded.

After scaling, the function clamps the resulting value between two bounds: a minimum threshold of one heap region size 
(\texttt{HeapRegion::GrainBytes}) and a maximum of the remaining uncommitted memory. 
The final value is the resize amount that FlexHeap passes onto the G1-Collector to resize its heap.

If a FlexHeap determines a shrink action is needed, the \texttt{young\_collection\_shrink\_amount()} is called, calculating a shrink amount,
that will be applied to the heap. Unlike the expand function, 
which dynamically scales based on CPU usage deltas, the shrink amount has a fixed maximum scaling factor, ensuring that the
shrink actions are more aggressive, in order to provide as much memory to the page-cache at any given moment. 
This factor is defined by the parameter \texttt{G1ShrinkByPercentOfAvailable}, 
which defaults to \texttt{50}, resulting in a maximum scale factor of \texttt{0.5}, or 50\% of the free memory regions,
while also leaving space for sudden allocation bursts.

To calculate the amount of memory to shrink, the function first finds the number of currently free heap 
regions via \texttt{\_g1h->num\_free\_regions()}. This value represents memory that is already committed but 
not actively being used. It then multiplies this number by the size of a single heap region (\texttt{HeapRegion::GrainBytes})
and scales it by the shrink factor. The result is the total number of bytes for a heap shrinkage.
%
% % interval is the time frame between 2 young gcs
% % overview section
% % how do i measure the gc overheads (CPU time spent on STW and CPU on concurrent and refinmenet threads)
% % how do we measure I/O
% % applying the smae resizing step as vanila g1 with different thresholds

