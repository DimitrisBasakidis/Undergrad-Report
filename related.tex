\section{Related Work}
\note{jk: the related work seems that is copy paste based on what I gave you.
This is not correct. You have to write your own text}
Prior approaches fall into four categories: cached heaps, tiered heaps, hybrid
heaps, and managed heap resizing techniques.

Cached heaps like Semeru \cite{semeru}, MemLiner \cite{memliner}, and Mako
\cite{mako} allocate the managed heap entirely on remote memory while using
local DRAM as a cache. These systems often require kernel modifications to
implement remote paging and suffer from high I/O traffic and GC overheads due to
frequent object scans and evacuations between memory tiers. For example, Semeru
offloads GC scans to remote JVM instances to mitigate latency, while MemLiner
reorganizes GC thread memory access order to align with mutator access patterns.
However, both still suffer from costly object transfers and rewriting between
memory tiers. Mako further introduces a Heap Indirection Table to track object
locations, which imposes load reference barriers on every access, adding CPU
overhead. Friendly-NVM-GC \cite{friendlynvmgc} uses DRAM as a cache for a heap
entirely placed on NVM but cannot avoid slow GC scans over NVM-resident objects.

Tiered heaps aim to reduce write amplification or NVM access latencies. Crystal
Gazer \cite{crystalgazer1, crystalgazer2} and GCMove \cite{gcmove} frequently
migrate objects between DRAM and NVM generations to limit writes but incur slow
scans to update references. ThinGC \cite{thingc} ensures mutators never directly
access NVM-resident objects by moving them into DRAM upon access, avoiding
immediate reference updates through lazy barriers, but this increases DRAM
memory pressure and GC frequency. JPDHeap \cite{jpdheap} requires programmers to
annotate objects for placement across DRAM and NVM, again leading to expensive
NVM scans for reference adjustments.

Hybrid heaps such as Melt \cite{melt} and LeakSurvivor \cite{leaksurvivor} place
hot objects in DRAM and cold objects on HDD or SSD, maintaining a user-space
cache inside the JVM to track object offsets. While effective for HDD latency,
they require a cache lookup for every access, adding CPU management overhead.
TeraHeap \cite{TeraHeap}, on the other hand, targets NVMe SSDs with
memory-mapped I/O to reduce cache lookup costs, but its DRAM partition between
the heap (H1) and page cache for the secondary heap (H2) is statically
configured and does not adapt to workload changes.

Finally, managed heap resizing techniques focus on improving GC performance by
dynamically adjusting heap sizes. Brecht et al. \cite{brecht} propose aggressive
heap expansion in Boehm GC to reduce execution time, while Yang et al.
\cite{yang} develop an analytical model for heap resizing in multi-tenant
environments. ElasticMem \cite{elasticmem} reclaims heap memory for other
processes in shared clusters to increase throughput. White et al. \cite{white}
use PID controllers to meet user-defined GC goals by tuning heap resize ratios.
However, these approaches solely optimize GC behavior without considering I/O
costs, limiting their applicability to hybrid heaps that require balanced DRAM
allocation between managed memory and page cache for performance stability.

In contrast to these systems, our Dynamic Heap Resizer monitors runtime GC
overhead and I/O cost, enabling dynamic partitioning of DRAM within hybrid
managed heaps.
